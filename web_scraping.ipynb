{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries\n",
    "\n",
    "from datetime import datetime\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from requests.exceptions import SSLError, Timeout, TooManyRedirects, RequestException\n",
    "from contextlib import closing\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "timeout_glob=2\n",
    "verify_glob=True\n",
    "allow_glob=False\n",
    "\n",
    "# Defining the headers to access the webpage\n",
    "\n",
    "headers = \"\"\"accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\n",
    "accept-encoding: gzip, deflate, br\n",
    "accept-language: pt-PT,pt;q=0.9,en-US;q=0.8,en;q=0.7,fr;q=0.6,cs;q=0.5,es;q=0.4\n",
    "cache-control: no-cache\n",
    "cookie: __cfduid=d70a2e1775967b7e98c88e5e8e0e9a22b1603275849; _ga=GA1.2.306171135.1603275850; _gid=GA1.2.301121098.1603275850; adbrgn=FRIDF; mp_eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjbGllbnROYW1lIjoid2liYml0eiIsImlucHV0TGFiZWwiOiJqc1RyYWNrZXIiLCJpbnB1dFR5cGUiOiJKU1NESyJ9.55Y_puhX1ed2hNYaK3G18PI5iuXXJJ_rEZSRVjfK7WE_alooma=%7B%22distinct_id%22%3A%20%221754ab0040e58d-05296d9c2063f9-c781f38-1fa400-1754ab0040f688%22%2C%22%24search_engine%22%3A%20%22google%22%2C%22%24initial_referrer%22%3A%20%22https%3A%2F%2Fwww.google.com%2F%22%2C%22%24initial_referring_domain%22%3A%20%22www.google.com%22%7D\n",
    "pragma: no-cache\n",
    "referer: https://www.politifact.com/\n",
    "sec-fetch-dest: document\n",
    "sec-fetch-mode: navigate\n",
    "sec-fetch-site: same-origin\n",
    "sec-fetch-user: ?1\n",
    "upgrade-insecure-requests: 1\n",
    "user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36\"\"\"\n",
    "headers=dict([i.strip().split(': ') for i in headers.split('\\n')])\n",
    "\n",
    "# Defining the error handling functions\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown('**'+string+'**'))\n",
    "def is_good_response(x):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be an HTML, False otherwise\n",
    "    x is response\n",
    "    \"\"\"\n",
    "    content_type=x.headers['Content-Type'].lower()\n",
    "    return ((x.status_code==200 and content_type is not None and content_type.find('html')>1), x.status_code)\n",
    "def log_error(e):\n",
    "    print(e)\n",
    "def status_handler(status):\n",
    "    if status<300:\n",
    "        print('Alles gut!')\n",
    "    elif status>=400 and status<500:\n",
    "        print('request has failed due to you mistake bro. Check the link, headers, server and whether you have an access to this page.')\n",
    "    else:\n",
    "        print('Yoooo, I have no clue what has happened. But you are stuck, since you didnt receive any output from server')\n",
    "    return None\n",
    "def adaptive_scraping(url, timeout=timeout_glob, verify=verify_glob):\n",
    "    global timeout_glob\n",
    "    global verify_glob\n",
    "    global allow_glob\n",
    "    import requests as r\n",
    "    try:\n",
    "        with closing(r.get(url, stream=True, timeout=timeout_glob, verify=verify_glob, allow_redirects=allow_glob, headers=headers)) as resp:\n",
    "            test1=is_good_response(resp)\n",
    "            print(resp)\n",
    "            if test1[0]:\n",
    "                return resp.content\n",
    "            else:\n",
    "                return status_handler(test1[1])\n",
    "    except Timeout as e:\n",
    "        log_error(f'Error during request to {url}: {str(e)}')\n",
    "        printmd('We can increase response waiting time. Do you want to continue?')\n",
    "        ans=input('\\n[Yes|No]\\t')\n",
    "        if ans.lower()=='yes':\n",
    "            timeout_glob*=2\n",
    "            return adaptive_scraping(url)\n",
    "        else:\n",
    "            return None\n",
    "    except SSLError as e:\n",
    "        log_error(f'Error during request to {url}: {str(e)}')\n",
    "        printmd('We can skip verification test. Do you want to continue?')\n",
    "        ans=input('\\n[Yes|No]\\t')\n",
    "        if ans.lower()=='yes':\n",
    "            verify_glob=False\n",
    "            return adaptive_scraping(url)\n",
    "        else:\n",
    "            return None\n",
    "    except TooManyRedirects as e:\n",
    "        log_error(f'Error during request to {url}: {str(e)}')\n",
    "        printmd('We can increase the number of allowed redirects. Do you want to continue?')\n",
    "        ans=input('\\n[Yes|No]\\t')\n",
    "        if ans.lower()=='yes':\n",
    "            allow_glob=True\n",
    "            return adaptive_scraping(url)\n",
    "        else:\n",
    "            return None\n",
    "    except RequestException as e:\n",
    "        log_error(f'Error during request to {url}: {str(e)}')\n",
    "        printmd('Unfortunately, we have no clue what to do.  Please try again later. ')\n",
    "        return None\n",
    "    \n",
    "# Defining the function to extract the information from the webpage\n",
    "\n",
    "def extract():\n",
    "\n",
    "    from datetime import datetime\n",
    "    pages = [*range(1,1000)]\n",
    "    ratings = []\n",
    "    prices = []\n",
    "    info = []\n",
    "    name = []\n",
    "    category = []\n",
    "    discount = []\n",
    "    stock = []\n",
    "\n",
    "    # Creating the loop to web scrape each page\n",
    "\n",
    "    for page in pages:\n",
    "        url = 'https://www.darty.com/nav/achat/gros_electromenager/lave-linge/page'+str(page)+'.html'\n",
    "        adaptive_scraping(url)\n",
    "        response = r.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        container = soup.select('div.product_main_container')\n",
    "        \n",
    "        if container ==[]:\n",
    "            break\n",
    "        else:\n",
    "\n",
    "            print('Retrieving data from page '+str(page))\n",
    "            sleep(randint(2,6))\n",
    "\n",
    "            # Append the information related to Ratings in the Ratings List\n",
    "\n",
    "            for i in container:\n",
    "                l=[i.text.strip() for i in i.select('span.rating_avis')]\n",
    "                if len(l)==0:\n",
    "                    ratings.append('0')\n",
    "                else:\n",
    "                    ratings.append(''.join(l))\n",
    "\n",
    "             # Append the information related to Prices in the Prices List\n",
    "\n",
    "            for i in container:\n",
    "                l=[i.text.strip() for i in i.select('span.darty_prix')]\n",
    "                if len(l)==0:\n",
    "                    prices.append('0')\n",
    "                else:\n",
    "                    prices.append(''.join(l))\n",
    "\n",
    "             # Append the information related to the features of the washer in the Info List\n",
    "\n",
    "            for i in container:\n",
    "                l=[i.text.strip() for i in i.select('ul.infos_strenghts')]\n",
    "                if len(l)==0:\n",
    "                    info.append('0')\n",
    "                else:\n",
    "                    info.append(''.join(l))\n",
    "\n",
    "             # Append the information related to the category, brand and model in the Name List\n",
    "\n",
    "            for i in container:\n",
    "                l=[i.text.strip() for i in i.select('div.prd-family')]\n",
    "                if len(l)==0:\n",
    "                    name.append('0')\n",
    "                else:\n",
    "                    name.append(''.join(l))\n",
    "\n",
    "             # Append the information related to Discounts in the Discounts List\n",
    "\n",
    "            for i in container:\n",
    "                l=[i.text.strip().replace('- ','') for i in i.select('span.striped_price')]\n",
    "                if len(l)==0:\n",
    "                    discount.append('0')\n",
    "                else:\n",
    "                    discount.append(''.join(l))\n",
    "\n",
    "             # Append the information related to stock in the stock List\n",
    "\n",
    "            for i in container:\n",
    "                l=[i.text.strip() for i in i.select('div.status_delivery p.delivery_date')]\n",
    "                if len(l)==0:\n",
    "                    stock.append('0')\n",
    "                else:\n",
    "                    stock.append(''.join(l))\n",
    "                \n",
    "    # Merge every list into one dataframe\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'Name': name,\n",
    "         'Info': info,\n",
    "         'Price': prices,\n",
    "         'Discount': discount,\n",
    "         'Stock': stock,\n",
    "         'Ratings': ratings\n",
    "        })\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
